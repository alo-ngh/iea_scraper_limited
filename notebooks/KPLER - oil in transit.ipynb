{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPLER API - oil in transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from numpy import int64\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"Basic ZGlvbnlzaWEubHluZ29wb3Vsb3VAaWVhLm9yZzpHbHJ6dDB6bg==\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list with the the time range of equally spaced time points (every 31 days)\n",
    "def date_time(starting_date,ending_date):\n",
    "    date_time_index=pd.date_range(start=starting_date, end=ending_date, freq='1M')\n",
    "    date_range=date_time_index.tolist()\n",
    "    return date_range\n",
    "    # print(type(date_range))\n",
    "    # print ( date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a data frame with vessels data for a specific period of time\n",
    "\n",
    "def loaded_vessels(date_range):\n",
    "    vessel_data = []\n",
    "    for n in range(len(date_range)):\n",
    "        end_date=date_range[n].date()\n",
    "        print(f\"Iteration {n}: {end_date}\")\n",
    "        endpoint = \"https://api.kpler.com/v1/fleet-metrics/vessels\"\n",
    "        payload={}\n",
    "        params_vesseldata = {\"metric\": \"loaded_vessels\",\n",
    "                         \"zones\":\"world\",\n",
    "                         \"floatingStorageDurationMin\":\"12\", \n",
    "                         \"floatingStorageDurationMax\":\"Inf\", \n",
    "                         \"period\": \"daily\", \n",
    "                         \"unit\":\"kb\",\n",
    "                         \"endDate\":end_date }\n",
    "        \n",
    "\n",
    "        response_vessel_data = requests.get(endpoint, params=params_vesseldata, headers=headers, data=payload, verify=False)\n",
    "        response_vessel_data_content = response_vessel_data.content\n",
    "        vessel_data_df = pd.read_csv(io.StringIO(response_vessel_data_content.decode('utf-8')), sep=';', parse_dates=[1], infer_datetime_format=True)\n",
    "        vessel_data.append(vessel_data_df)\n",
    "    vessel_data = pd.concat(vessel_data)\n",
    "    return vessel_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Historical data (2016- previous year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "starting_date ='2016-01-01'\n",
    "ending_date = today - relativedelta(months=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LYNGOPOULOU_D\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:979: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.kpler.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f9a9fa905032>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdate_range_his\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstarting_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mending_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloaded_vessels_hist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloaded_vessels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_range_his\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mloaded_vessels_hist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\LYNGOPOULOU_D\\PycharmProjects\\scraper\\filestore\\oil_transit_hist.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-94888aa60483>\u001b[0m in \u001b[0;36mloaded_vessels\u001b[1;34m(date_range)\u001b[0m\n\u001b[0;32m     17\u001b[0m                          \"endDate\":{end_date} }\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mresponse_vessel_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams_vesseldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mresponse_vessel_data_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse_vessel_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mvessel_data_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_vessel_data_content\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    424\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1332\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "date_range_his=date_time(starting_date,ending_date)\n",
    "loaded_vessels_hist=loaded_vessels(date_range_his)\n",
    "loaded_vessels_hist.to_csv(r'C:\\Users\\LYNGOPOULOU_D\\PycharmProjects\\scraper\\filestore\\oil_transit_hist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Current data ( lastyear - today)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "starting_date = today - relativedelta(months=12)\n",
    "ending_date = today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LYNGOPOULOU_D\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:979: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.kpler.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LYNGOPOULOU_D\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:979: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.kpler.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "date_range=date_time(starting_date,ending_date)\n",
    "loaded_vessels=loaded_vessels(date_range)\n",
    "loaded_vessels.to_csv(r'C:\\Users\\LYNGOPOULOU_D\\PycharmProjects\\scraper\\filestore\\oil_transit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:scraper.core.factory:Loading module scraper.jobs.com_kpler.oil_in_transit\n",
      "DEBUG:scraper.core.factory:Getting class OilInTransitJob\n",
      "INFO:scraper.core.job:Temporary table name: #oil_in_transit_temp, final table name: oil_in_transit_data\n",
      "DEBUG:scraper.jobs.com_kpler.oil_in_transit:Assuming parallel_download=False as default to avoid Kpler overload.\n",
      "INFO:scraper.jobs.com_kpler.oil_in_transit:Getting sources...\n",
      "INFO:scraper.jobs.com_kpler.oil_in_transit:6 sources to load.\n",
      "INFO:scraper.core.utils:download_and_get_checksum: 84.5029354095459 ms\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: processing com_kpler_transit_2021-10-31\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"GET /dimension/source?code=com_kpler_transit_2021-10-31 HTTP/1.1\" 200 825\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: processing com_kpler_transit_2021-09-30\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"GET /dimension/source?code=com_kpler_transit_2021-09-30 HTTP/1.1\" 200 825\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: removing com_kpler_transit_2021-09-30 from self.sources\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: processing com_kpler_transit_2021-08-31\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"GET /dimension/source?code=com_kpler_transit_2021-08-31 HTTP/1.1\" 200 825\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: removing com_kpler_transit_2021-08-31 from self.sources\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: processing com_kpler_transit_2021-07-31\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"GET /dimension/source?code=com_kpler_transit_2021-07-31 HTTP/1.1\" 200 825\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: removing com_kpler_transit_2021-07-31 from self.sources\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: processing com_kpler_transit_2021-06-30\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"GET /dimension/source?code=com_kpler_transit_2021-06-30 HTTP/1.1\" 200 825\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: removing com_kpler_transit_2021-06-30 from self.sources\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: processing com_kpler_transit_2021-05-31\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"GET /dimension/source?code=com_kpler_transit_2021-05-31 HTTP/1.1\" 200 825\n",
      "DEBUG:scraper.core.job:rm_sources_up_to_date: removing com_kpler_transit_2021-05-31 from self.sources\n",
      "INFO:scraper.core.utils:rm_sources_up_to_date: 199.1100311279297 ms\n",
      "DEBUG:scraper.core.job:Adding sources to dynamic_dim['source']...\n",
      "DEBUG:scraper.core.job:1 sources added to self.dynamic_dim['source']\n",
      "DEBUG:scraper.core.job:remove_existing_dynamic_dim: query - http://vipenta.iea.org:8000/dimension/source\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"GET /dimension/source HTTP/1.1\" 200 896116\n",
      "DEBUG:scraper.core.job:self.dynamic_dim['source'] size before: 1\n",
      "DEBUG:scraper.core.job:self.dynamic_dim['source'] size after: 0\n",
      "DEBUG:scraper.core.job:Transforming provider ...\n",
      "DEBUG:scraper.core.job:Adding provider to dynamic_dim: COM_KPLER\n",
      "DEBUG:scraper.core.job:remove_existing_dynamic_dim: query - http://vipenta.iea.org:8000/dimension/provider\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"GET /dimension/provider HTTP/1.1\" 200 8530\n",
      "DEBUG:scraper.core.job:self.dynamic_dim['provider'] size before: 1\n",
      "DEBUG:scraper.core.job:self.dynamic_dim['provider'] size after: 0\n",
      "INFO:scraper.jobs.com_kpler.oil_in_transit:Transforming data\n",
      "INFO:scraper.jobs.com_kpler.oil_in_transit:30018 rows read from C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_2021-10-31.csv\n",
      "DEBUG:scraper.core.job:Running insert_new_dynamic_dim(): 2 items\n",
      "DEBUG:scraper.core.job:Processing source: size 0\n",
      "DEBUG:scraper.core.job:Processing provider: size 0\n",
      "INFO:scraper.core.utils:insert_new_dynamic_dim: 3.0007362365722656 ms\n",
      "INFO:scraper.core.job:Writing to database.\n",
      "DEBUG:scraper.core.job:Database: mssql+pyodbc://omr:Sekiyu8trd@vipenta.iea.org/external_db_dev?driver=ODBC+Driver+13+for+SQL+Server\n",
      "DEBUG:scraper.core.job:Loading 30018 rows to temporary table #oil_in_transit_temp\n",
      "DEBUG:scraper.core.job:Merging data from #oil_in_transit_temp into oil_in_transit_data\n",
      "DEBUG:scraper.core.job:Merge query: MERGE kpler.oil_in_transit_data target \n",
      "USING kpler.#oil_in_transit_temp as source \n",
      "ON (target.[provider] = source.[provider] AND target.[source] = source.[source] AND target.[Date] = source.[Date] AND target.[IMO] = source.[IMO] AND target.[Product] = source.[Product] AND target.[Grade] = source.[Grade]) \n",
      "WHEN MATCHED AND (target.[Name] <> source.[Name] OR target.[Dead Weight Tonnage] <> source.[Dead Weight Tonnage] OR target.[Quantity (kb)] <> source.[Quantity (kb)] OR target.[Family] <> source.[Family] OR target.[Group] <> source.[Group] OR target.[Current Continent] <> source.[Current Continent] OR target.[Current Subcontinent] <> source.[Current Subcontinent] OR target.[Current Country] <> source.[Current Country] OR target.[Current Sea] <> source.[Current Sea])\n",
      "THEN UPDATE SET target.[Name] = source.[Name], target.[Dead Weight Tonnage] = source.[Dead Weight Tonnage], target.[Quantity (kb)] = source.[Quantity (kb)], target.[Family] = source.[Family], target.[Group] = source.[Group], target.[Current Continent] = source.[Current Continent], target.[Current Subcontinent] = source.[Current Subcontinent], target.[Current Country] = source.[Current Country], target.[Current Sea] = source.[Current Sea], target.[date_modified] = GETDATE()\n",
      "WHEN NOT MATCHED \n",
      "THEN INSERT ([Date], [IMO], [Name], [Dead Weight Tonnage], [Quantity (kb)], [Family], [Group], [Product], [Grade], [Current Continent], [Current Subcontinent], [Current Country], [Current Sea], [provider], [source], [date_created]) \n",
      "VALUES (source.[Date], source.[IMO], source.[Name], source.[Dead Weight Tonnage], source.[Quantity (kb)], source.[Family], source.[Group], source.[Product], source.[Grade], source.[Current Continent], source.[Current Subcontinent], source.[Current Country], source.[Current Sea], source.[provider], source.[source], GETDATE());\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"GET /dimension/source?code=com_kpler_transit_2021-10-31 HTTP/1.1\" 200 825\n",
      "ERROR:scraper.core.job:Exception in update_sources_metadata(): 'BaseSource' object has no attribute 'last_download'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\scraper\\core\\job.py\", line 875, in update_sources_metadata\n",
      "    data[\"last_download\"] = source.last_download\n",
      "AttributeError: 'BaseSource' object has no attribute 'last_download'\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): vipenta.iea.org:8000\n",
      "DEBUG:urllib3.connectionpool:http://vipenta.iea.org:8000 \"PUT /dimension/source/7228 HTTP/1.1\" 201 10\n",
      "INFO:scraper.core.utils:update_sources_metadata: 59.111595153808594 ms\n"
     ]
    }
   ],
   "source": [
    "from scraper.core import factory\n",
    "\n",
    "#job = factory.get_scraper_job('com_kpler', 'oil_in_transit', full_load=True)\n",
    "job = factory.get_scraper_job('com_kpler', 'oil_in_transit')\n",
    "#job.run()\n",
    "# job.get_sources()\n",
    "job.run(download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-01-31.csv\n",
      "Renaming com_kpler_transit_data_2016-01-31.csv to com_kpler_transit_2016-01-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-02-29.csv\n",
      "Renaming com_kpler_transit_data_2016-02-29.csv to com_kpler_transit_2016-02-29.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-03-31.csv\n",
      "Renaming com_kpler_transit_data_2016-03-31.csv to com_kpler_transit_2016-03-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-04-30.csv\n",
      "Renaming com_kpler_transit_data_2016-04-30.csv to com_kpler_transit_2016-04-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-05-31.csv\n",
      "Renaming com_kpler_transit_data_2016-05-31.csv to com_kpler_transit_2016-05-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-06-30.csv\n",
      "Renaming com_kpler_transit_data_2016-06-30.csv to com_kpler_transit_2016-06-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-07-31.csv\n",
      "Renaming com_kpler_transit_data_2016-07-31.csv to com_kpler_transit_2016-07-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-08-31.csv\n",
      "Renaming com_kpler_transit_data_2016-08-31.csv to com_kpler_transit_2016-08-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-09-30.csv\n",
      "Renaming com_kpler_transit_data_2016-09-30.csv to com_kpler_transit_2016-09-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-10-31.csv\n",
      "Renaming com_kpler_transit_data_2016-10-31.csv to com_kpler_transit_2016-10-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-11-30.csv\n",
      "Renaming com_kpler_transit_data_2016-11-30.csv to com_kpler_transit_2016-11-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2016-12-31.csv\n",
      "Renaming com_kpler_transit_data_2016-12-31.csv to com_kpler_transit_2016-12-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-01-31.csv\n",
      "Renaming com_kpler_transit_data_2017-01-31.csv to com_kpler_transit_2017-01-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-02-28.csv\n",
      "Renaming com_kpler_transit_data_2017-02-28.csv to com_kpler_transit_2017-02-28.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-03-31.csv\n",
      "Renaming com_kpler_transit_data_2017-03-31.csv to com_kpler_transit_2017-03-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-04-30.csv\n",
      "Renaming com_kpler_transit_data_2017-04-30.csv to com_kpler_transit_2017-04-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-05-31.csv\n",
      "Renaming com_kpler_transit_data_2017-05-31.csv to com_kpler_transit_2017-05-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-06-30.csv\n",
      "Renaming com_kpler_transit_data_2017-06-30.csv to com_kpler_transit_2017-06-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-07-31.csv\n",
      "Renaming com_kpler_transit_data_2017-07-31.csv to com_kpler_transit_2017-07-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-08-31.csv\n",
      "Renaming com_kpler_transit_data_2017-08-31.csv to com_kpler_transit_2017-08-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-09-30.csv\n",
      "Renaming com_kpler_transit_data_2017-09-30.csv to com_kpler_transit_2017-09-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-10-31.csv\n",
      "Renaming com_kpler_transit_data_2017-10-31.csv to com_kpler_transit_2017-10-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-11-30.csv\n",
      "Renaming com_kpler_transit_data_2017-11-30.csv to com_kpler_transit_2017-11-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2017-12-31.csv\n",
      "Renaming com_kpler_transit_data_2017-12-31.csv to com_kpler_transit_2017-12-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-01-31.csv\n",
      "Renaming com_kpler_transit_data_2018-01-31.csv to com_kpler_transit_2018-01-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-02-28.csv\n",
      "Renaming com_kpler_transit_data_2018-02-28.csv to com_kpler_transit_2018-02-28.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-03-31.csv\n",
      "Renaming com_kpler_transit_data_2018-03-31.csv to com_kpler_transit_2018-03-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-04-30.csv\n",
      "Renaming com_kpler_transit_data_2018-04-30.csv to com_kpler_transit_2018-04-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-05-31.csv\n",
      "Renaming com_kpler_transit_data_2018-05-31.csv to com_kpler_transit_2018-05-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-06-30.csv\n",
      "Renaming com_kpler_transit_data_2018-06-30.csv to com_kpler_transit_2018-06-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-07-31.csv\n",
      "Renaming com_kpler_transit_data_2018-07-31.csv to com_kpler_transit_2018-07-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-08-31.csv\n",
      "Renaming com_kpler_transit_data_2018-08-31.csv to com_kpler_transit_2018-08-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-09-30.csv\n",
      "Renaming com_kpler_transit_data_2018-09-30.csv to com_kpler_transit_2018-09-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-10-31.csv\n",
      "Renaming com_kpler_transit_data_2018-10-31.csv to com_kpler_transit_2018-10-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-11-30.csv\n",
      "Renaming com_kpler_transit_data_2018-11-30.csv to com_kpler_transit_2018-11-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2018-12-31.csv\n",
      "Renaming com_kpler_transit_data_2018-12-31.csv to com_kpler_transit_2018-12-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-01-31.csv\n",
      "Renaming com_kpler_transit_data_2019-01-31.csv to com_kpler_transit_2019-01-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-02-28.csv\n",
      "Renaming com_kpler_transit_data_2019-02-28.csv to com_kpler_transit_2019-02-28.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-03-31.csv\n",
      "Renaming com_kpler_transit_data_2019-03-31.csv to com_kpler_transit_2019-03-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-04-30.csv\n",
      "Renaming com_kpler_transit_data_2019-04-30.csv to com_kpler_transit_2019-04-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-05-31.csv\n",
      "Renaming com_kpler_transit_data_2019-05-31.csv to com_kpler_transit_2019-05-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-06-30.csv\n",
      "Renaming com_kpler_transit_data_2019-06-30.csv to com_kpler_transit_2019-06-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-07-31.csv\n",
      "Renaming com_kpler_transit_data_2019-07-31.csv to com_kpler_transit_2019-07-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-08-31.csv\n",
      "Renaming com_kpler_transit_data_2019-08-31.csv to com_kpler_transit_2019-08-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-09-30.csv\n",
      "Renaming com_kpler_transit_data_2019-09-30.csv to com_kpler_transit_2019-09-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-10-31.csv\n",
      "Renaming com_kpler_transit_data_2019-10-31.csv to com_kpler_transit_2019-10-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-11-30.csv\n",
      "Renaming com_kpler_transit_data_2019-11-30.csv to com_kpler_transit_2019-11-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2019-12-31.csv\n",
      "Renaming com_kpler_transit_data_2019-12-31.csv to com_kpler_transit_2019-12-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-01-31.csv\n",
      "Renaming com_kpler_transit_data_2020-01-31.csv to com_kpler_transit_2020-01-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-02-29.csv\n",
      "Renaming com_kpler_transit_data_2020-02-29.csv to com_kpler_transit_2020-02-29.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-03-31.csv\n",
      "Renaming com_kpler_transit_data_2020-03-31.csv to com_kpler_transit_2020-03-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-04-30.csv\n",
      "Renaming com_kpler_transit_data_2020-04-30.csv to com_kpler_transit_2020-04-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-05-31.csv\n",
      "Renaming com_kpler_transit_data_2020-05-31.csv to com_kpler_transit_2020-05-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-06-30.csv\n",
      "Renaming com_kpler_transit_data_2020-06-30.csv to com_kpler_transit_2020-06-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-07-31.csv\n",
      "Renaming com_kpler_transit_data_2020-07-31.csv to com_kpler_transit_2020-07-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-08-31.csv\n",
      "Renaming com_kpler_transit_data_2020-08-31.csv to com_kpler_transit_2020-08-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-09-30.csv\n",
      "Renaming com_kpler_transit_data_2020-09-30.csv to com_kpler_transit_2020-09-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-10-31.csv\n",
      "Renaming com_kpler_transit_data_2020-10-31.csv to com_kpler_transit_2020-10-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-11-30.csv\n",
      "Renaming com_kpler_transit_data_2020-11-30.csv to com_kpler_transit_2020-11-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2020-12-31.csv\n",
      "Renaming com_kpler_transit_data_2020-12-31.csv to com_kpler_transit_2020-12-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-01-31.csv\n",
      "Renaming com_kpler_transit_data_2021-01-31.csv to com_kpler_transit_2021-01-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-02-28.csv\n",
      "Renaming com_kpler_transit_data_2021-02-28.csv to com_kpler_transit_2021-02-28.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-03-31.csv\n",
      "Renaming com_kpler_transit_data_2021-03-31.csv to com_kpler_transit_2021-03-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-04-30.csv\n",
      "Renaming com_kpler_transit_data_2021-04-30.csv to com_kpler_transit_2021-04-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-05-31.csv\n",
      "Renaming com_kpler_transit_data_2021-05-31.csv to com_kpler_transit_2021-05-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-06-30.csv\n",
      "Renaming com_kpler_transit_data_2021-06-30.csv to com_kpler_transit_2021-06-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-07-31.csv\n",
      "Renaming com_kpler_transit_data_2021-07-31.csv to com_kpler_transit_2021-07-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-08-31.csv\n",
      "Renaming com_kpler_transit_data_2021-08-31.csv to com_kpler_transit_2021-08-31.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-09-30.csv\n",
      "Renaming com_kpler_transit_data_2021-09-30.csv to com_kpler_transit_2021-09-30.csv\n",
      "done!\n",
      "C:\\Users\\ROSA_L\\PycharmProjects\\scraper\\filestore\\com_kpler_transit_data_2021-10-31.csv\n",
      "Renaming com_kpler_transit_data_2021-10-31.csv to com_kpler_transit_2021-10-31.csv\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "from scraper.settings import FILE_STORE_PATH\n",
    "\n",
    "for file in FILE_STORE_PATH.glob('com_kpler_transit_data*'):\n",
    "    print(file)\n",
    "    name = file.name\n",
    "    suffix = name.split('data')[1]\n",
    "    new_name = f'com_kpler_transit{suffix}'\n",
    "    print(f'Renaming {name} to {new_name}')\n",
    "    file.rename(new_name)\n",
    "    print('done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): api-oil.kpler.com:80\n",
      "DEBUG:urllib3.connectionpool:http://api-oil.kpler.com:80 \"GET /v1/fleet-metrics/vessels?metric=loaded_vessels&zones=world&floatingStorageDurationMin=12&floatingStorageDurationMax=Inf&period=daily&unit=kb&endDate=2021-01-31 HTTP/1.1\" 301 134\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api-oil.kpler.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing method 1:\n",
      "Testing method 2:\n",
      "Testing method 3:\n",
      "reading chunk...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"http://api-oil.kpler.com/v1/fleet-metrics/vessels?metric=loaded_vessels&zones=world&floatingStorageDurationMin=12&floatingStorageDurationMax=Inf&period=daily&unit=kb&endDate=2020-10-31\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "print('Testing method 1:')\n",
    "r = requests.get(url, headers=headers)\n",
    "\n",
    "r.raise_for_status()\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "f = Path('test1.csv')\n",
    "\n",
    "with f.open(mode='wb') as fp:\n",
    "    fp.write(r.content)\n",
    "    \n",
    "print('Testing method 2:')    \n",
    "r = requests.get(url, headers=headers)\n",
    "\n",
    "r.raise_for_status()\n",
    "\n",
    "f2 = Path('test2.csv')\n",
    "f2.write_bytes(r.content)\n",
    "\n",
    "chunk_size = 2048\n",
    "print('Testing method 3:')  \n",
    "r = requests.get(url, headers=headers, stream=True)\n",
    "f3 = Path('test3.csv')\n",
    "with f3.open(mode='wb') as fd:\n",
    "    print('reading chunk...')\n",
    "    for chunk in r.iter_content(chunk_size):\n",
    "        fd.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 1: test1.csv \n",
      " 0    Dirty\n",
      "2      DPP\n",
      "Name: Family, dtype: object\n",
      "file 2: test2.csv\n",
      " 0    Dirty\n",
      "2      DPP\n",
      "Name: Family, dtype: object\n",
      "file 3: test3.csv\n",
      " 0    Dirty\n",
      "2      DPP\n",
      "Name: Family, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(f, sep=';')\n",
    "print(f\"file 1: {f} \\n {df1['Family'].drop_duplicates()}\")\n",
    "df2 = pd.read_csv(f2, sep=';')\n",
    "print(f\"file 2: {f2}\\n {df2['Family'].drop_duplicates()}\")\n",
    "df3 = pd.read_csv(f3, sep=';')\n",
    "print(f\"file 3: {f3}\\n {df3['Family'].drop_duplicates()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 1: test1.csv \n",
      " 0                  Crude\n",
      "2        Bitumen/Asphalt\n",
      "4                     FO\n",
      "6                    NaN\n",
      "61            Condensate\n",
      "101                  VGO\n",
      "151                 SRFO\n",
      "515                 CBFS\n",
      "1357              Slurry\n",
      "13704       Specialities\n",
      "Name: Product, dtype: object\n",
      "file 2: test2.csv\n",
      " 0                  Crude\n",
      "2        Bitumen/Asphalt\n",
      "4                     FO\n",
      "6                    NaN\n",
      "61            Condensate\n",
      "101                  VGO\n",
      "151                 SRFO\n",
      "515                 CBFS\n",
      "1357              Slurry\n",
      "13704       Specialities\n",
      "Name: Product, dtype: object\n",
      "file 3: test3.csv\n",
      " 0                  Crude\n",
      "2        Bitumen/Asphalt\n",
      "4                     FO\n",
      "6                    NaN\n",
      "61            Condensate\n",
      "101                  VGO\n",
      "151                 SRFO\n",
      "515                 CBFS\n",
      "1357              Slurry\n",
      "13704       Specialities\n",
      "Name: Product, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(f, sep=';')\n",
    "print(f\"file 1: {f} \\n {df1['Product'].drop_duplicates()}\")\n",
    "df2 = pd.read_csv(f2, sep=';')\n",
    "print(f\"file 2: {f2}\\n {df2['Product'].drop_duplicates()}\")\n",
    "df3 = pd.read_csv(f3, sep=';')\n",
    "print(f\"file 3: {f3}\\n {df3['Product'].drop_duplicates()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 'loaded_vessels', 'zones': 'world', 'floatingStorageDurationMin': '12', 'floatingStorageDurationMax': 'Inf', 'period': 'daily', 'unit': 'kb', 'endDate': {'2020-10-31'}}\n",
      "get_params.csv: 6222631 bytes written.\n",
      "get_inline.csv: 6222631 bytes written.\n"
     ]
    }
   ],
   "source": [
    "# test inline params x params parameter\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "headers = {\"Authorization\": \"Basic ZGlvbnlzaWEubHluZ29wb3Vsb3VAaWVhLm9yZzpHbHJ6dDB6bg==\"}\n",
    "end_date = \"2020-10-31\"\n",
    "base_url = \"http://api-oil.kpler.com/v1/fleet-metrics/vessels\"\n",
    "\n",
    "params_vesseldata = {\"metric\": \"loaded_vessels\",\n",
    "                     \"zones\":\"world\",\n",
    "                     \"floatingStorageDurationMin\":\"12\", \n",
    "                     \"floatingStorageDurationMax\":\"Inf\", \n",
    "                     \"period\": \"daily\", \n",
    "                     \"unit\":\"kb\",\n",
    "                     \"endDate\": {end_date} }\n",
    "\n",
    "print(params_vesseldata)\n",
    "\n",
    "r = requests.get(base_url, params=params_vesseldata, headers=headers)\n",
    "file1 = Path('get_params.csv')\n",
    "r.raise_for_status()\n",
    "file1.write_bytes(r.content)\n",
    "print(f'{file1}: {len(r.content)} bytes written.')\n",
    "    \n",
    "url = (\"http://api-oil.kpler.com/v1/fleet-metrics/vessels?\"\n",
    "       \"metric=loaded_vessels&\"\n",
    "       \"zones=world&\"\n",
    "       \"floatingStorageDurationMin=12&\"\n",
    "       \"floatingStorageDurationMax=Inf&\"\n",
    "       \"period=daily&\"\n",
    "       \"unit=kb&endDate=2020-10-31\")\n",
    "\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "file2 = Path('get_inline.csv')\n",
    "r.raise_for_status()\n",
    "file2.write_bytes(r.content)\n",
    "print(f'{file2}: {len(r.content)} bytes written.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 2021-10-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rosa_l\\pycharmprojects\\scraper\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:857: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_text.csv: 118258 rows written.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date = datetime(2021, 10, 31)\n",
    "\n",
    "df = loaded_vessels([date])\n",
    "\n",
    "print(f'{file2}: {len(df)} rows written.')\n",
    "\n",
    "df.to_csv('file_load_vessels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: None\n",
      "Apparent Encoding: ascii\n",
      "test_binary.csv: 6222600 bytes written.\n",
      "Encoding: None\n",
      "Apparent Encoding: ascii\n",
      "test_text.csv: 6222600 bytes written.\n",
      "Encoding: None\n",
      "Apparent Encoding: ascii\n",
      "test_encode.csv: 6222600 bytes written.\n"
     ]
    }
   ],
   "source": [
    "# Test using decode('utf-8') and text\n",
    "url = (\"http://api-oil.kpler.com/v1/fleet-metrics/vessels?\"\n",
    "       \"metric=loaded_vessels&\"\n",
    "       \"zones=world&\"\n",
    "       \"floatingStorageDurationMin=12&\"\n",
    "       \"floatingStorageDurationMax=Inf&\"\n",
    "       \"period=daily&\"\n",
    "       \"unit=kb&endDate=2020-10-31\")\n",
    "\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "file2 = Path('test_binary.csv')\n",
    "r.raise_for_status()\n",
    "print(f'Encoding: {r.encoding}')\n",
    "print(f'Apparent Encoding: {r.apparent_encoding}')\n",
    "file2.write_bytes(r.content)\n",
    "print(f'{file2}: {len(r.content)} bytes written.')\n",
    "\n",
    "\n",
    "file2 = Path('test_text.csv')\n",
    "r.raise_for_status()\n",
    "print(f'Encoding: {r.encoding}')\n",
    "print(f'Apparent Encoding: {r.apparent_encoding}')\n",
    "file2.write_text(r.text)\n",
    "print(f'{file2}: {len(r.text)} bytes written.')\n",
    "\n",
    "file3 = Path('test_encode.csv')\n",
    "r.raise_for_status()\n",
    "print(f'Encoding: {r.encoding}')\n",
    "print(f'Apparent Encoding: {r.apparent_encoding}')\n",
    "file3.write_text(r.content.decode('utf-8'))\n",
    "print(f'{file3}: {len(r.text)} bytes written.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: None\n",
      "Apparent Encoding: ascii\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "data must be str, not StringIO",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5af2013e97c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Encoding: {r.encoding}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Apparent Encoding: {r.apparent_encoding}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfile3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{file3}: {len(r.text)} bytes written.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36mwrite_text\u001b[1;34m(self, data, encoding, errors)\u001b[0m\n\u001b[0;32m   1210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m             raise TypeError('data must be str, not %s' %\n\u001b[1;32m-> 1212\u001b[1;33m                             data.__class__.__name__)\n\u001b[0m\u001b[0;32m   1213\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: data must be str, not StringIO"
     ]
    }
   ],
   "source": [
    "import io\n",
    "file3 = Path('binary_encode.csv')\n",
    "r.raise_for_status()\n",
    "print(f'Encoding: {r.encoding}')\n",
    "print(f'Apparent Encoding: {r.apparent_encoding}')\n",
    "\n",
    "decoded = \n",
    "\n",
    "file3.write_text(io.StringIO(r.content.decode('utf-8')))\n",
    "print(f'{file3}: {len(r.text)} bytes written.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: None\n",
      "Apparent Encoding: ascii\n",
      "binary_encode.csv: 6222600 bytes written.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "file3 = Path('binary_encode.csv')\n",
    "r.raise_for_status()\n",
    "print(f'Encoding: {r.encoding}')\n",
    "print(f'Apparent Encoding: {r.apparent_encoding}')\n",
    "\n",
    "file3.write_text(io.StringIO(r.content.decode('utf-8')).getvalue())\n",
    "print(f'{file3}: {len(r.text)} bytes written.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pandas\n",
    "\n",
    "vessel_data_df = pd.read_csv(io.StringIO(r.content.decode('utf-8')), sep=';', parse_dates=[1], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Dirty\n",
       "2      DPP\n",
       "Name: Family, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vessel_data_df['Family'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.kpler.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2021-10-31\n",
      "This works:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rosa_l\\pycharmprojects\\scraper\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:857: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "DEBUG:urllib3.connectionpool:https://api.kpler.com:443 \"GET /v1/fleet-metrics/vessels?metric=loaded_vessels&zones=world&floatingStorageDurationMin=12&floatingStorageDurationMax=Inf&period=daily&unit=kb&endDate=2021-10-31 HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                  Dirty\n",
       "1                    DPP\n",
       "2     Middle Distillates\n",
       "9             Light Ends\n",
       "11                 Clean\n",
       "12                   NPC\n",
       "Name: Family, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.kpler.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This does not work:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rosa_l\\pycharmprojects\\scraper\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:857: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "DEBUG:urllib3.connectionpool:https://api.kpler.com:443 \"GET /v1/fleet-metrics/vessels?metric=loaded_vessels&zones=world&floatingStorageDurationMin=12&floatingStorageDurationMax=Inf&period=daily&unit=kb&endDate=2020-10-31 HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                  Dirty\n",
       "2                    DPP\n",
       "7     Middle Distillates\n",
       "10            Light Ends\n",
       "12                   NPC\n",
       "14                 Clean\n",
       "Name: Family, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# url = (\"https://api-oil.kpler.com/v1/fleet-metrics/vessels?\"\n",
    "url = (\"https://api.kpler.com/v1/fleet-metrics/vessels?\"\n",
    "       \"metric=loaded_vessels&\"\n",
    "       \"zones=world&\"\n",
    "       \"floatingStorageDurationMin=12&\"\n",
    "       \"floatingStorageDurationMax=Inf&\"\n",
    "       \"period=daily&\"\n",
    "       \"unit=kb&endDate=2020-10-31\")\n",
    "\n",
    "\n",
    "end_date = datetime(2021, 10, 31).date()\n",
    "#end_date=date_range[n].date()\n",
    "print(f\"Date: {end_date}\")\n",
    "endpoint = \"https://api.kpler.com/v1/fleet-metrics/vessels\"\n",
    "payload={}\n",
    "params_vesseldata = {\"metric\": \"loaded_vessels\",\n",
    "                 \"zones\":\"world\",\n",
    "                 \"floatingStorageDurationMin\":\"12\", \n",
    "                 \"floatingStorageDurationMax\":\"Inf\", \n",
    "                 \"period\": \"daily\", \n",
    "                 \"unit\":\"kb\",\n",
    "                 \"endDate\":end_date}\n",
    "\n",
    "# This is NOK\n",
    "#r = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "# This one is OK\n",
    "print('This works:')\n",
    "r = requests.get(endpoint, params=params_vesseldata, headers=headers, data=payload, verify=False)\n",
    "r.raise_for_status()\n",
    "df = pd.read_csv(io.StringIO(r.text), sep=';', encoding=\"utf-8\", parse_dates=[1], infer_datetime_format=True)\n",
    "\n",
    "display(df['Family'].drop_duplicates())\n",
    "\n",
    "# This doesn't work\n",
    "print('This does not work:')\n",
    "r = requests.get(url, headers=headers, data=payload, verify=False)\n",
    "r.raise_for_status()\n",
    "df = pd.read_csv(io.StringIO(r.text), sep=';', encoding=\"utf-8\", parse_dates=[1], infer_datetime_format=True)\n",
    "\n",
    "display(df['Family'].drop_duplicates())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So the culprit for those differences was... the endpoint!\n",
    "\n",
    "We should use https://api.kpler.com instead https://api-oil.kpler.com, unless we want only oil data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13250826 bytes written.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13250826"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file = Path('binary_encode_test.csv')\n",
    "print(f\"{len(r.content)} bytes written.\")\n",
    "file.write_bytes(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  Dirty\n",
       "1                    DPP\n",
       "2     Middle Distillates\n",
       "9             Light Ends\n",
       "11                 Clean\n",
       "12                   NPC\n",
       "Name: Family, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(file, sep=';')['Family'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
